{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #put a space between punctuation, so not included in word\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #remove things that are not letters or punctuation\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    # basically removes non-english standard alphabet letters and replaces with equivalent.\n",
    "    # so no accented letters => less to learn\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def readLangs(input_file_location, lang1_name=\"English\", lang2_name=\"French\"):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(input_file_location, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    input_lang = Lang(lang1_name)\n",
    "    output_lang = Lang(lang2_name)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make learning easier, we will limit to sentences less than max length \n",
    "# in either language\n",
    "# And only take sentences that start with certain prefixes\n",
    "# Removed punctuation b/c filtered out in our normalize function\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "def prepareData(input_file_location, lang1_name=\"English\", \n",
    "                lang2_name=\"French\"):\n",
    "    input_lang, output_lang, pairs = readLangs(input_file_location, lang1_name,\n",
    "                                              lang2_name)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of sentence\n",
    "SOS_token = 0\n",
    "# end of sentence\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 149861 sentence pairs\n",
      "Trimmed to 11589 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "English 3016\n",
      "French 4634\n",
      "['he is clumsy with his hands .', 'il est maladroit de ses mains .']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(\"../../../data/text/fra-eng/fra.txt\")\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    # make it 1 column with number of rows equal to words in sentence\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def variableFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    output_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, output_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple encoder network that embeds the character and then feeds through a GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Attn Decoder\n",
    "    1. Need max length because learning which input words to attend to\n",
    "    And thus need to know the maximum number of words could attend to\n",
    "    2. The attn_weights tell us how much to weight each input word - in this case French,\n",
    "       In order to predict the english word.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # note input and output same size\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.attn_layer = nn.Linear(2 * self.hidden_size, MAX_LENGTH)\n",
    "        self.out_layer = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.attn_combined_layer = nn.Linear(2 * self.hidden_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn = self.attn_layer(torch.cat((embedded[0], hidden[0]),dim=1))\n",
    "        attn_weights = self.softmax(attn)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), \n",
    "                                encoder_outputs.unsqueeze(0))\n",
    "        attn_combined = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        attn_combined = self.relu(self.attn_combined_layer(attn_combined))\n",
    "        output, hidden = self.gru(attn_combined, hidden)\n",
    "        output = self.softmax(self.out_layer(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer,\n",
    "         decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    loss = 0\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    \n",
    "    # Here we are feeding in the english words to get the final hidden state \n",
    "    # for the decoder\n",
    "    for i in range(input_length):\n",
    "        encoder_ouput, encoder_hidden = encoder.forward(input_variable[i], encoder_hidden)\n",
    "        \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    # Here we take the final hidden state from the encoder\n",
    "    # And feed it to decoder\n",
    "    # We also give decoder the word to predict the next word starting with SOS token\n",
    "    # If use teacher forcing then give it the truth, otherwise give it prediction\n",
    "    if use_teacher_forcing:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder.forward(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[i])\n",
    "            decoder_input = target_variable[i]\n",
    "            \n",
    "    else:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder.forward(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[i])\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            top_predicted = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[top_predicted]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            if top_predicted == EOS_token:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100,\n",
    "               learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #get our training data\n",
    "    training_pairs = [variableFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter-1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "        \n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 16s (- 31m 45s) (5000 6%) 3.5147\n",
      "4m 31s (- 29m 25s) (10000 13%) 2.9476\n",
      "6m 49s (- 27m 16s) (15000 20%) 2.6149\n",
      "9m 7s (- 25m 4s) (20000 26%) 2.3670\n",
      "11m 26s (- 22m 52s) (25000 33%) 2.1233\n",
      "13m 45s (- 20m 37s) (30000 40%) 1.9208\n",
      "16m 4s (- 18m 22s) (35000 46%) 1.7785\n",
      "18m 23s (- 16m 5s) (40000 53%) 1.6309\n",
      "20m 44s (- 13m 49s) (45000 60%) 1.4921\n",
      "23m 3s (- 11m 31s) (50000 66%) 1.3803\n",
      "25m 23s (- 9m 13s) (55000 73%) 1.2808\n",
      "27m 43s (- 6m 55s) (60000 80%) 1.1962\n",
      "30m 3s (- 4m 37s) (65000 86%) 1.0892\n",
      "32m 23s (- 2m 18s) (70000 93%) 1.0039\n",
      "34m 43s (- 0m 0s) (75000 100%) 0.9792\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = DecoderRNN(output_lang.n_words, hidden_size)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "trainIters(encoder, decoder, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"../models/encoder.state\")\n",
    "torch.save(decoder.state_dict(), \"../models/decoder.state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "\n",
    "    for i in range(input_length):\n",
    "        encoder_ouput, encoder_hidden = encoder.forward(input_variable[i], encoder_hidden)\n",
    "        \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoded_words = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder.forward(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        top_predicted = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[top_predicted]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        if top_predicted == EOS_token:\n",
    "            decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[top_predicted])\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\">\", pair[0])\n",
    "        print(\"=\", pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        print(\"<\", \" \".join(output_words))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> i m worried .\n",
      "= je me fais du souci .\n",
      "< je fais du souci . <EOS>\n",
      "\n",
      "> i m really flattered to hear that .\n",
      "= je suis tres flatte d entendre cela .\n",
      "< je suis vraiment d d entendre cela . <EOS>\n",
      "\n",
      "> i m very much in favor of this .\n",
      "= je suis tres en faveur de cela .\n",
      "< je suis tres en en faveur de . . <EOS>\n",
      "\n",
      "> i m going to miss you .\n",
      "= vous allez me manquer .\n",
      "< tu vas me manquer . <EOS>\n",
      "\n",
      "> you re not very funny .\n",
      "= tu n es pas tres amusant .\n",
      "< vous n etes pas tres amusante . <EOS>\n",
      "\n",
      "> i m not giving up yet .\n",
      "= je ne vais pas encore abandonner .\n",
      "< je ne gobe pas encore ca . <EOS>\n",
      "\n",
      "> i m not mad .\n",
      "= je ne suis pas fou .\n",
      "< je ne suis pas fou . <EOS>\n",
      "\n",
      "> they re not all busy .\n",
      "= elles ne sont pas toutes occupees .\n",
      "< ils ne sont pas tous occupes . <EOS>\n",
      "\n",
      "> you re not a loser .\n",
      "= vous n etes pas un perdant .\n",
      "< vous n etes pas un perdant . <EOS>\n",
      "\n",
      "> i m going to hit the hay .\n",
      "= je vais me pieuter .\n",
      "< je vais me pieuter . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'vais', 'me', 'battre', '.', '<EOS>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, \"i m going to teach .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
