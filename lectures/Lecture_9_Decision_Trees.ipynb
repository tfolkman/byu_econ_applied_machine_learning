{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "To better understand decision trees, let's take a look at a simple example:\n",
    "\n",
    "https://zhengtianyu.files.wordpress.com/2013/12/decision-tree.png\n",
    "\n",
    "Clearly, decision trees are very simple and pretty easy to understand. Once you have a tree, you just follow the forks down for your example to get to an end node. Then at that end node you take the most common class or average value for regression. In fact, with classification you can get probabilistic estimates by taking the fraction of examples in that class. Nice! And that is one of the biggest benefits of decision trees - they are so easy to explain and understand.\n",
    "\n",
    "Most decision trees use binary trees - each split is either yes or no. When you graph these decision boundaries you end up with a bunch of vertical or horizontal lines. Here is an example with the iris data set:\n",
    "\n",
    "http://statweb.stanford.edu/~jtaylo/courses/stats202/trees.html\n",
    "\n",
    "## How To Grow A Tree\n",
    "\n",
    "So - how does one learn a tree? Do we just randomly pick binary splitting points and see what comes out? Of course not! We leverage our data and a definition of impurity. \n",
    "\n",
    "If you think about it, what we really want from our tree is pure leaf nodes. Meaning that for classification we would like at the end to have the leaf node only have examples of a single class. This greatly increases our confidence that if a testing example ends up at that leaf node that it is in fact of that class.\n",
    "\n",
    "Imagine the opposite, a leaf node in a 2 class classification problem that has 50% of the examples in one class and 50% in the other. That really doesn't help us at all! If a testing example ends up at that leaf node, all we can say is a 50/50 guess.\n",
    "\n",
    "So, how do we define impurity?\n",
    "\n",
    "**Gini Impurity**\n",
    "\n",
    "$$Gini_{i} = 1 - \\sum_{k=1}^{n}{p_{i,k}^2}$$\n",
    "\n",
    "Where $i$ is the node of interest, $n$ is the total number of classes, and $p_{i,k}$ is the fraction of class $k$ in node $i$. The gini of a node is 0 when all the examples belong to the same class. Gini is the highest when there is an equal probability of being in each class. \n",
    "\n",
    "Let's consider a 2-class examples. Say I am trying to predict whether someone is male or female and I branch on height being less than 5 1/2 feet. In this node, I find 50 examples of which 40 are female and 10 are male. Gini is then:\n",
    "\n",
    "$$1 - (40/50)^2 - (10/50)^2 = 0.32$$\n",
    "\n",
    "Very nice - now we can basically evaluate how good a node is. With this we can now start to **greedily** grow our tree. What I mean by greedily is that we will not consider every possible tree, but instead start with the most pure feature split and then from there pick the next most pure, etc.\n",
    "\n",
    "Thus, our algorithm looks at all the features and all the splitting points for our features (note: this process runs much faster if you features have a small amount of unique values like categories as opposed to real numbers). To evalue a feature, split point pair we do the following:\n",
    "\n",
    "Take the weighted average of the Gini impurity for the two nodes created by the split. The weights being the number of examples in the nodes.\n",
    "\n",
    "That's it! Basically, the feature split that produces the lowest weighted average gini impurity is considered best, then we move on and find the next best feature split given all the previous feature splits.\n",
    "\n",
    "### When to stop?\n",
    "\n",
    "Decision trees have many hyper-parameters to help control when to stop growing the tree. These include max depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes.\n",
    "\n",
    "Since we have to consider all these splits, training can be slow, but predictions are very fast since we just have to traverse the tree.\n",
    "\n",
    "Also, there are other impurity measures that are used with decision trees. Another very popular one is Entropy. See page 173 of Hands On Machine Learning for its definition. Usually, which you choose doesn't matter too much. Gini is slightly faster to computer, while entropy produces slightly more balanced trees.\n",
    "\n",
    "### Bias and variance\n",
    "\n",
    "Decision trees can be very prone to overfitting if you let them grow to deep. Thus, decreasing the depth can decrease variance / increase bias. It is important to use cross-validation to do hyper-parameter selection. \n",
    "\n",
    "### Regression\n",
    "\n",
    "Decision trees can be applied to regression problems in exactly the same way, but instead of using Gini impurity you would use mean squared error. You calculate the MSE of a node by setting the prediction for all values in that node as the average $y$ value of the examples in that node. For example, if your node had these values: 5, 2, 1, 6 then your predicted value would be (5+2+1+6)/4 = 3.5. You would do the squared difference between 3.5 and all the values and take the mean. \n",
    "\n",
    "Prediction is done by traversing to the leaf node for an example and taking the average value at that node.\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Easy to explain\n",
    "* Can be visualized\n",
    "* Can handle categorical variables and missing data well\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Typically don't have very strong prediction accuracy\n",
    "* Very sensitive to small changes in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
